{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3c437c",
   "metadata": {},
   "source": [
    "# Capstone â€” Minimal Viable Submission\n",
    "\n",
    "This single notebook implements the **minimal** end-to-end pipeline required for the capstone:\n",
    "\n",
    "- Minimal cleaning\n",
    "- Required EDA plots (5)\n",
    "- Baseline forecasting (RandomForest) for total daily sales (next 30 days)\n",
    "- Simple store segmentation (KMeans)\n",
    "- Save outputs for submission\n",
    "\n",
    "Follow cells in order and run everything. Paths and outputs are saved under `/mnt/data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a02a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and paths\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "\n",
    "DATA_PATH = '/mnt/data/Retail_Sales_Data_Unlox_(1)[1].csv'\n",
    "OUTPUT_DIR = '/mnt/data/capstone_outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print('Output dir:', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfa656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Rows, cols:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8579888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal cleaning (only what's necessary)\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "# Remove rows with invalid dates (if any)\n",
    "df = df.dropna(subset=['Date']).copy()\n",
    "# Ensure numeric columns\n",
    "for c in ['Unit_Price','Units_Sold','Total_Sales','Discount_Percentage','Revenue','Stock_On_Hand','Store_Rating']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "# Fill small missing numeric values with 0 for simplicity\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df[num_cols] = df[num_cols].fillna(0)\n",
    "# Basic derived columns\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "df['Is_Weekend'] = df['DayOfWeek'].isin([5,6]).astype(int)\n",
    "\n",
    "# Save cleaned sample\n",
    "cleaned_path = os.path.join(OUTPUT_DIR, 'processed_sales_minimal.csv')\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "print('Cleaned saved to', cleaned_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: create aggregate daily sales (Total_Sales) and required plots\n",
    "daily = df.groupby('Date', as_index=False)['Total_Sales'].sum().sort_values('Date')\n",
    "daily = daily.rename(columns={'Total_Sales':'Daily_Sales'})\n",
    "\n",
    "# 1. Monthly sales trend\n",
    "monthly = daily.copy()\n",
    "monthly['Month'] = monthly['Date'].dt.to_period('M')\n",
    "monthly_agg = monthly.groupby('Month')['Daily_Sales'].sum().reset_index()\n",
    "plt.figure(figsize=(10,4)); plt.plot(monthly_agg['Month'].astype(str), monthly_agg['Daily_Sales']); plt.xticks(rotation=45)\n",
    "plt.title('Monthly Sales Trend'); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,'plot_monthly_sales.png')); plt.close()\n",
    "\n",
    "# 2. Category-wise sales (top 10)\n",
    "cat_sales = df.groupby('Product_Category', as_index=False)['Total_Sales'].sum().sort_values('Total_Sales', ascending=False).head(10)\n",
    "plt.figure(figsize=(8,4)); plt.bar(cat_sales['Product_Category'], cat_sales['Total_Sales']); plt.xticks(rotation=45)\n",
    "plt.title('Top 10 Product Categories by Sales'); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,'plot_category_sales.png')); plt.close()\n",
    "\n",
    "# 3. Region-wise sales\n",
    "region_sales = df.groupby('Region', as_index=False)['Total_Sales'].sum()\n",
    "plt.figure(figsize=(6,4)); plt.bar(region_sales['Region'], region_sales['Total_Sales']); plt.title('Region-wise Sales'); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,'plot_region_sales.png')); plt.close()\n",
    "\n",
    "# 4. Promotion vs Non-promotion sales\n",
    "promo = df.copy()\n",
    "promo['Promotion_Applied'] = promo['Promotion_Applied'].astype(str).str.lower().map({'yes':1,'no':0}).fillna(0).astype(int)\n",
    "promo_sales = promo.groupby('Promotion_Applied')['Total_Sales'].sum().reset_index()\n",
    "plt.figure(figsize=(5,4)); plt.bar(['No','Yes'], promo_sales['Total_Sales']); plt.title('Promotion vs Non-Promotion Sales'); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,'plot_promo_sales.png')); plt.close()\n",
    "\n",
    "# 5. Discount vs Units_Sold scatter (sample)\n",
    "plt.figure(figsize=(6,4)); \n",
    "sample = df.sample(n=min(2000,len(df)), random_state=42)\n",
    "plt.scatter(sample['Discount_Percentage'], sample['Units_Sold'], alpha=0.3, s=10)\n",
    "plt.xlabel('Discount %'); plt.ylabel('Units Sold'); plt.title('Discount % vs Units Sold (sample)'); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,'plot_discount_units.png')); plt.close()\n",
    "\n",
    "print('EDA plots saved to', OUTPUT_DIR)\n",
    "monthly_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4befa1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting: create lag features on daily aggregate\n",
    "daily = daily.set_index('Date').asfreq('D').fillna(0).reset_index()\n",
    "daily['y'] = daily['Daily_Sales']\n",
    "\n",
    "# create lag features\n",
    "for lag in [1,7,14,30]:\n",
    "    daily[f'lag_{lag}'] = daily['y'].shift(lag).fillna(0)\n",
    "\n",
    "# rolling means\n",
    "daily['rmean_7'] = daily['y'].rolling(7,min_periods=1).mean().shift(1).fillna(0)\n",
    "daily['rmean_30'] = daily['y'].rolling(30,min_periods=1).mean().shift(1).fillna(0)\n",
    "\n",
    "# add day of week, month\n",
    "daily['dow'] = daily['Date'].dt.dayofweek\n",
    "daily['month'] = daily['Date'].dt.month\n",
    "\n",
    "daily = daily.fillna(0)\n",
    "daily.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split: last 60 days for testing, rest for training\n",
    "train = daily.iloc[:-60].copy()\n",
    "test = daily.iloc[-60:].copy()\n",
    "\n",
    "FEATURES = [col for col in train.columns if col not in ['Date','y']]\n",
    "X_train, y_train = train[FEATURES], train['y']\n",
    "X_test, y_test = test[FEATURES], test['y']\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=2)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_test = rf.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, pred_test)\n",
    "rmse = mean_squared_error(y_test, pred_test, squared=False)\n",
    "print(f'Test MAE: {mae:.2f}, RMSE: {rmse:.2f}')\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(OUTPUT_DIR, 'rf_daily_sales_model.pkl')\n",
    "with open(model_path,'wb') as f:\n",
    "    pickle.dump(rf, f)\n",
    "print('Saved model to', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fca1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast next 30 days using iterative strategy\n",
    "last_row = daily.iloc[-1:].copy()\n",
    "future = []\n",
    "cur = last_row.copy()\n",
    "for i in range(30):\n",
    "    X_cur = cur[FEATURES].iloc[0:1]\n",
    "    y_pred = rf.predict(X_cur)[0]\n",
    "    next_date = cur['Date'].iloc[0] + pd.Timedelta(days=1)\n",
    "    new_row = {'Date': next_date, 'y': y_pred}\n",
    "    # compute new lags/rolling quickly using previous rows: append to daily-like frame\n",
    "    cur = pd.DataFrame([new_row])\n",
    "    # compute features based on previous daily with simplistic approach\n",
    "    prev = daily.append(cur, ignore_index=True).iloc[-31:].copy()\n",
    "    prev = prev.reset_index(drop=True)\n",
    "    # build features for the new last row\n",
    "    row = prev.iloc[-1].copy()\n",
    "    for lag in [1,7,14,30]:\n",
    "        lag_val = prev['y'].shift(lag).iloc[-1]\n",
    "        row[f'lag_{lag}'] = 0 if pd.isna(lag_val) else lag_val\n",
    "    row['rmean_7'] = prev['y'].rolling(7,min_periods=1).mean().iloc[-2] if len(prev)>=2 else prev['y'].iloc[-1]\n",
    "    row['rmean_30'] = prev['y'].rolling(30,min_periods=1).mean().iloc[-2] if len(prev)>=2 else prev['y'].iloc[-1]\n",
    "    row['dow'] = next_date.dayofweek\n",
    "    row['month'] = next_date.month\n",
    "    # set cur to this constructed row\n",
    "    cur = pd.DataFrame([row])\n",
    "    daily = daily.append(cur, ignore_index=True)\n",
    "\n",
    "future_forecast = daily.tail(30)[['Date','y']].rename(columns={'y':'forecast'}).reset_index(drop=True)\n",
    "future_forecast.to_csv(os.path.join(OUTPUT_DIR,'forecast_next_30_days.csv'), index=False)\n",
    "print('Forecast saved to', os.path.join(OUTPUT_DIR,'forecast_next_30_days.csv'))\n",
    "future_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73371e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple store segmentation\n",
    "store_agg = df.groupby('Store_ID').agg(Avg_Sales=('Total_Sales','mean'),\n",
    "                                       Avg_Discount=('Discount_Percentage','mean'),\n",
    "                                       Avg_Stock=('Stock_On_Hand','mean')).reset_index().fillna(0)\n",
    "X_cluster = store_agg[['Avg_Sales','Avg_Discount','Avg_Stock']].copy()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "store_agg['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "store_agg.to_csv(os.path.join(OUTPUT_DIR,'store_segmentation.csv'), index=False)\n",
    "print('Store segmentation saved to', os.path.join(OUTPUT_DIR,'store_segmentation.csv'))\n",
    "store_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ea5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a minimal Streamlit app skeleton (not executed here)\n",
    "app_code = '''import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "st.set_page_config(layout='wide', page_title='Retail Analytics - Minimal')\n",
    "\n",
    "st.title('Retail Analytics & Forecasting - Minimal Submission')\n",
    "\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    df = pd.read_csv('/mnt/data/capstone_outputs/processed_sales_minimal.csv')\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "st.header('Overview')\n",
    "col1, col2, col3 = st.columns(3)\n",
    "col1.metric('Total Sales', f\"{df['Total_Sales'].sum():,.0f}\")\n",
    "col2.metric('Total Units', f\"{df['Units_Sold'].sum():,.0f}\")\n",
    "col3.metric('Unique Stores', df['Store_ID'].nunique())\n",
    "\n",
    "st.header('Forecast (next 30 days)')\n",
    "fc = pd.read_csv('/mnt/data/capstone_outputs/forecast_next_30_days.csv')\n",
    "st.line_chart(fc.set_index('Date')['forecast'])\n",
    "\n",
    "st.header('Store Segmentation Sample')\n",
    "seg = pd.read_csv('/mnt/data/capstone_outputs/store_segmentation.csv')\n",
    "st.dataframe(seg.head(20))\n",
    "'''\n",
    "\n",
    "os.makedirs('/mnt/data/capstone_outputs', exist_ok=True)\n",
    "with open('/mnt/data/dashboard_app_minimal.py','w') as f:\n",
    "    f.write(app_code)\n",
    "print('Streamlit app skeleton saved to /mnt/data/dashboard_app_minimal.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6109d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List outputs and show quick samples\n",
    "import os\n",
    "out = '/mnt/data/capstone_outputs'\n",
    "print('Files in output dir:')\n",
    "print(os.listdir(out))\n",
    "print('\\nForecast sample:')\n",
    "print(pd.read_csv(os.path.join(out,'forecast_next_30_days.csv')).head())\n",
    "print('\\nStore segmentation sample:')\n",
    "print(pd.read_csv(os.path.join(out,'store_segmentation.csv')).head())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
